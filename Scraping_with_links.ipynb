{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scraping with links",
      "provenance": [],
      "authorship_tag": "ABX9TyPtthiYJJBE84noY9ZvLtiF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/James-Sutanto/Web-Scraper/blob/main/Scraping_with_links.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-gFvvnJ7UTb"
      },
      "source": [
        "!pip install newspaper3k \n",
        "!pip install pygooglenews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMXgs7xO7ng7"
      },
      "source": [
        "import newspaper\n",
        "import pandas as pd\n",
        "def ss(url):\n",
        "  urls = []\n",
        "  for url in url:\n",
        "    url_i = newspaper.Article(url=\"%s\" % (url), language='en')\n",
        "    url_i.download()\n",
        "    url_i.parse()\n",
        "    urls.append(url_i.text)\n",
        "  urls = pd.DataFrame(urls)\n",
        "  return urls\n",
        "from pygooglenews import GoogleNews\n",
        "\n",
        "def gns(topic):\n",
        "  urls= []\n",
        "  gn = GoogleNews()\n",
        "  search = gn.search(topic)\n",
        "  newsitem = search['entries']\n",
        "  for item in newsitem:\n",
        "    urls.append(item['link'])\n",
        "  return urls\n",
        "\n",
        "def remove(urllist,link_name):\n",
        "  urllist.index(link_name)\n",
        "  print(urllist.index(link_name))\n",
        "  urllist.pop(urllist.index(link_name))\n",
        "\n",
        "def clean_df(df,df1):\n",
        "  df = df.replace([\"\\n\"],\" \",regex=True)\n",
        "  df = pd.concat([df,df1],axis=1)\n",
        "  return df\n",
        "\n",
        "import requests \n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def find_links_who(url):\n",
        "  who_url = [] \n",
        "  html = requests.get(url)\n",
        "  \n",
        "  if html == 200:\n",
        "   soup = BeautifulSoup(html.text,'lxml')\n",
        "   divs = soup.find_all('div', attrs={\"class\":\"sf-list-vertical__item\"})\n",
        "   for div_tag in divs: \n",
        "    a_tag = div_tag.find('a')\n",
        "    who_url.append(a_tag.attrs['href'])\n",
        " \n",
        "   else:\n",
        "     print(\"The website has been blocked\")\n",
        "\n",
        "def find_links_pp(url):\n",
        "  pp_url = [] \n",
        "  for page in range(1,11):\n",
        "   html = requests.get(url + str(page) )\n",
        "   if (html == 200):\n",
        "    soup = BeautifulSoup(html.text,'lxml')\n",
        "    h3_tag = soup.find_all('h3')\n",
        "    for h3 in h3_tag: \n",
        "     a_tag = div_tag.find('a')\n",
        "     pp_url.append(a_tag.attrs['href'])\n",
        " \n",
        "    else:\n",
        "     print(\"The website has been blocked\")\n",
        "\n",
        "def find_links_yw(url):\n",
        "  urls = []\n",
        "for x in range(1,9):\n",
        "  html = requests.get('https://youngwomenshealth.org/category/guides/page/'+str(x)+\n",
        "                     '/?topic=gyn&s=Menstrual+Health','lxml')\n",
        "\n",
        "  soup = BeautifulSoup(html.text,'lxml')\n",
        "  h3_tag = soup.find_all('h3', attrs={\"class\":\"h2\"})\n",
        " \n",
        "  for h3 in h3_tag: \n",
        "   a_tag = h3.find('a')\n",
        "   urls.append(a_tag.attrs['href'])\n",
        "\n",
        "def fl_nwhn():\n",
        "  nwhn = []\n",
        "for x in range(1,4):\n",
        "  html = requests.get('https://nwhn.org/search-results/?_sf_s=Menstruation&sf_paged=' + str(x))\n",
        "  soup = BeautifulSoup(html.text, 'lxml')\n",
        "  containers = soup.find_all('div', class_ = \"inner\" )\n",
        "  for container in containers:\n",
        "    div = container.find('div', class_=\"info\")\n",
        "    h4 = container.find('h4')\n",
        "    a = h4.find('a')\n",
        "    href = a.attrs['href']\n",
        "    nwhn.append(href)\n",
        "print(nwhn)\n",
        "len(nwhn)\n",
        "\n",
        "dep= []\n",
        "depd ={} \n",
        "for x in range(1,4):\n",
        "  html = requests.get(\"https://www.mentalhealthjournal.org/all-articles?page=\"+str(x))\n",
        "  soup = BeautifulSoup(html.text, 'lxml')\n",
        "  articles = soup.find_all('h3', class_= 'page-header1' )\n",
        "\n",
        "  for article in articles:\n",
        "    a = article.find('a')\n",
        "    href = a.attrs['href']\n",
        "    dep.append(href)\n",
        "len(dep)\n",
        "dep_d = {\"Links\": dep}\n",
        "print(dep_d)\n",
        "\n",
        "anx= [] \n",
        "for x in range(0,41):\n",
        "  if x%10==0 :\n",
        "    html = requests.get(\"https://www.verywellmind.com/search?q=anxiety+disorder\"+ str(x))\n",
        "    soup = BeautifulSoup(html.text, 'lxml')\n",
        "    articles = soup.find_all('li', class_ = \"loc item search-result-list-item\") \n",
        "    for article in articles:\n",
        "     a = article.find('a')\n",
        "     href = a.attrs['href']\n",
        "     anx.append(href)\n",
        "len(anx)\n",
        "print(anx)\n",
        "\n",
        "def rename_columns():\n",
        "col = list(cd.columns)\n",
        "col[0] = 'Article'\n",
        "col[1] = 'Links'\n",
        "cd.columns = col"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}